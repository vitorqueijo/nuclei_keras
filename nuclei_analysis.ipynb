{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "nuclei_analysis.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/vitorqueijo/nuclei_keras/blob/master/nuclei_analysis.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lLl8Afwvozip",
        "colab_type": "text"
      },
      "source": [
        "# Pixels's intensity of positive and negative nuclei\n",
        "## dataset to detect 20x20 pixel nuclei in histological images\n",
        "\n",
        "Dataset's source: https://www.kaggle.com/zicouc/pixelss-intensity-of-positive-and-negative-nuclei/\n",
        "\n",
        "by VÃ­tor Queijo, Science and Technology graduation student"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U4N-v2JSq4wN",
        "colab_type": "text"
      },
      "source": [
        "In this easy example, we're going to use Keras, a neural network library fast and simple to implement. On this dataset, it's perceptible the similarity with the classical MNIST exercise, one of the first datasets you will use at the beginning of your ML course. So, it's going to be the same implematation with minor changes in the parameters and preprocessing. I hope it's useful in the future to any other grad student (I spent almost 4 hours to understand how to get from DataFrame to Tensor object)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AB3SKKRthAqB",
        "colab_type": "code",
        "outputId": "78fc4b24-296f-46b1-df9f-a03d825ac506",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import keras\n",
        "from keras.layers.core import Dense, Dropout, Activation, Flatten\n",
        "from keras.layers.convolutional import Convolution2D, MaxPooling2D\n",
        "from keras.models import Sequential\n",
        "from sklearn.model_selection import train_test_split"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZMW68auTsWMp",
        "colab_type": "text"
      },
      "source": [
        "## First, let's see with pandas what we're dealing with"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "outputId": "f4364b7e-b08f-4193-9c8a-a8040c8202df",
        "id": "AoVEsMZvwJBI",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 256
        }
      },
      "source": [
        "train_df: pd.DataFrame = pd.read_csv('data.csv')\n",
        "train_df.head()"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>pixel1</th>\n",
              "      <th>pixel2</th>\n",
              "      <th>pixel3</th>\n",
              "      <th>pixel4</th>\n",
              "      <th>pixel5</th>\n",
              "      <th>pixel6</th>\n",
              "      <th>pixel7</th>\n",
              "      <th>pixel8</th>\n",
              "      <th>pixel9</th>\n",
              "      <th>pixel10</th>\n",
              "      <th>pixel11</th>\n",
              "      <th>pixel12</th>\n",
              "      <th>pixel13</th>\n",
              "      <th>pixel14</th>\n",
              "      <th>pixel15</th>\n",
              "      <th>pixel16</th>\n",
              "      <th>pixel17</th>\n",
              "      <th>pixel18</th>\n",
              "      <th>pixel19</th>\n",
              "      <th>pixel20</th>\n",
              "      <th>pixel21</th>\n",
              "      <th>pixel22</th>\n",
              "      <th>pixel23</th>\n",
              "      <th>pixel24</th>\n",
              "      <th>pixel25</th>\n",
              "      <th>pixel26</th>\n",
              "      <th>pixel27</th>\n",
              "      <th>pixel28</th>\n",
              "      <th>pixel29</th>\n",
              "      <th>pixel30</th>\n",
              "      <th>pixel31</th>\n",
              "      <th>pixel32</th>\n",
              "      <th>pixel33</th>\n",
              "      <th>pixel34</th>\n",
              "      <th>pixel35</th>\n",
              "      <th>pixel36</th>\n",
              "      <th>pixel37</th>\n",
              "      <th>pixel38</th>\n",
              "      <th>pixel39</th>\n",
              "      <th>pixel40</th>\n",
              "      <th>...</th>\n",
              "      <th>pixel362</th>\n",
              "      <th>pixel363</th>\n",
              "      <th>pixel364</th>\n",
              "      <th>pixel365</th>\n",
              "      <th>pixel366</th>\n",
              "      <th>pixel367</th>\n",
              "      <th>pixel368</th>\n",
              "      <th>pixel369</th>\n",
              "      <th>pixel370</th>\n",
              "      <th>pixel371</th>\n",
              "      <th>pixel372</th>\n",
              "      <th>pixel373</th>\n",
              "      <th>pixel374</th>\n",
              "      <th>pixel375</th>\n",
              "      <th>pixel376</th>\n",
              "      <th>pixel377</th>\n",
              "      <th>pixel378</th>\n",
              "      <th>pixel379</th>\n",
              "      <th>pixel380</th>\n",
              "      <th>pixel381</th>\n",
              "      <th>pixel382</th>\n",
              "      <th>pixel383</th>\n",
              "      <th>pixel384</th>\n",
              "      <th>pixel385</th>\n",
              "      <th>pixel386</th>\n",
              "      <th>pixel387</th>\n",
              "      <th>pixel388</th>\n",
              "      <th>pixel389</th>\n",
              "      <th>pixel390</th>\n",
              "      <th>pixel391</th>\n",
              "      <th>pixel392</th>\n",
              "      <th>pixel393</th>\n",
              "      <th>pixel394</th>\n",
              "      <th>pixel395</th>\n",
              "      <th>pixel396</th>\n",
              "      <th>pixel397</th>\n",
              "      <th>pixel398</th>\n",
              "      <th>pixel399</th>\n",
              "      <th>pixel400</th>\n",
              "      <th>Label</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>84</td>\n",
              "      <td>72</td>\n",
              "      <td>85</td>\n",
              "      <td>104</td>\n",
              "      <td>89</td>\n",
              "      <td>59</td>\n",
              "      <td>47</td>\n",
              "      <td>48</td>\n",
              "      <td>69</td>\n",
              "      <td>88</td>\n",
              "      <td>112</td>\n",
              "      <td>137</td>\n",
              "      <td>146</td>\n",
              "      <td>124</td>\n",
              "      <td>102</td>\n",
              "      <td>102</td>\n",
              "      <td>120</td>\n",
              "      <td>120</td>\n",
              "      <td>102</td>\n",
              "      <td>86</td>\n",
              "      <td>79</td>\n",
              "      <td>70</td>\n",
              "      <td>75</td>\n",
              "      <td>89</td>\n",
              "      <td>86</td>\n",
              "      <td>69</td>\n",
              "      <td>60</td>\n",
              "      <td>64</td>\n",
              "      <td>79</td>\n",
              "      <td>84</td>\n",
              "      <td>94</td>\n",
              "      <td>115</td>\n",
              "      <td>125</td>\n",
              "      <td>106</td>\n",
              "      <td>87</td>\n",
              "      <td>92</td>\n",
              "      <td>120</td>\n",
              "      <td>121</td>\n",
              "      <td>107</td>\n",
              "      <td>92</td>\n",
              "      <td>...</td>\n",
              "      <td>128</td>\n",
              "      <td>92</td>\n",
              "      <td>80</td>\n",
              "      <td>84</td>\n",
              "      <td>74</td>\n",
              "      <td>74</td>\n",
              "      <td>67</td>\n",
              "      <td>69</td>\n",
              "      <td>75</td>\n",
              "      <td>71</td>\n",
              "      <td>62</td>\n",
              "      <td>62</td>\n",
              "      <td>60</td>\n",
              "      <td>66</td>\n",
              "      <td>81</td>\n",
              "      <td>79</td>\n",
              "      <td>83</td>\n",
              "      <td>89</td>\n",
              "      <td>92</td>\n",
              "      <td>116</td>\n",
              "      <td>118</td>\n",
              "      <td>88</td>\n",
              "      <td>80</td>\n",
              "      <td>93</td>\n",
              "      <td>94</td>\n",
              "      <td>90</td>\n",
              "      <td>70</td>\n",
              "      <td>64</td>\n",
              "      <td>67</td>\n",
              "      <td>60</td>\n",
              "      <td>56</td>\n",
              "      <td>60</td>\n",
              "      <td>58</td>\n",
              "      <td>60</td>\n",
              "      <td>72</td>\n",
              "      <td>90</td>\n",
              "      <td>99</td>\n",
              "      <td>114</td>\n",
              "      <td>130</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>196</td>\n",
              "      <td>161</td>\n",
              "      <td>153</td>\n",
              "      <td>174</td>\n",
              "      <td>188</td>\n",
              "      <td>206</td>\n",
              "      <td>169</td>\n",
              "      <td>171</td>\n",
              "      <td>149</td>\n",
              "      <td>140</td>\n",
              "      <td>147</td>\n",
              "      <td>177</td>\n",
              "      <td>200</td>\n",
              "      <td>200</td>\n",
              "      <td>190</td>\n",
              "      <td>185</td>\n",
              "      <td>178</td>\n",
              "      <td>162</td>\n",
              "      <td>131</td>\n",
              "      <td>99</td>\n",
              "      <td>181</td>\n",
              "      <td>168</td>\n",
              "      <td>161</td>\n",
              "      <td>164</td>\n",
              "      <td>174</td>\n",
              "      <td>208</td>\n",
              "      <td>182</td>\n",
              "      <td>175</td>\n",
              "      <td>142</td>\n",
              "      <td>148</td>\n",
              "      <td>169</td>\n",
              "      <td>199</td>\n",
              "      <td>213</td>\n",
              "      <td>208</td>\n",
              "      <td>201</td>\n",
              "      <td>202</td>\n",
              "      <td>214</td>\n",
              "      <td>205</td>\n",
              "      <td>185</td>\n",
              "      <td>161</td>\n",
              "      <td>...</td>\n",
              "      <td>142</td>\n",
              "      <td>135</td>\n",
              "      <td>140</td>\n",
              "      <td>138</td>\n",
              "      <td>132</td>\n",
              "      <td>142</td>\n",
              "      <td>137</td>\n",
              "      <td>149</td>\n",
              "      <td>145</td>\n",
              "      <td>145</td>\n",
              "      <td>141</td>\n",
              "      <td>159</td>\n",
              "      <td>174</td>\n",
              "      <td>159</td>\n",
              "      <td>161</td>\n",
              "      <td>155</td>\n",
              "      <td>133</td>\n",
              "      <td>132</td>\n",
              "      <td>146</td>\n",
              "      <td>128</td>\n",
              "      <td>137</td>\n",
              "      <td>132</td>\n",
              "      <td>138</td>\n",
              "      <td>132</td>\n",
              "      <td>119</td>\n",
              "      <td>127</td>\n",
              "      <td>125</td>\n",
              "      <td>132</td>\n",
              "      <td>129</td>\n",
              "      <td>128</td>\n",
              "      <td>120</td>\n",
              "      <td>140</td>\n",
              "      <td>157</td>\n",
              "      <td>142</td>\n",
              "      <td>142</td>\n",
              "      <td>125</td>\n",
              "      <td>113</td>\n",
              "      <td>129</td>\n",
              "      <td>159</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>138</td>\n",
              "      <td>157</td>\n",
              "      <td>172</td>\n",
              "      <td>152</td>\n",
              "      <td>140</td>\n",
              "      <td>139</td>\n",
              "      <td>133</td>\n",
              "      <td>151</td>\n",
              "      <td>158</td>\n",
              "      <td>140</td>\n",
              "      <td>131</td>\n",
              "      <td>136</td>\n",
              "      <td>139</td>\n",
              "      <td>134</td>\n",
              "      <td>131</td>\n",
              "      <td>134</td>\n",
              "      <td>148</td>\n",
              "      <td>146</td>\n",
              "      <td>148</td>\n",
              "      <td>157</td>\n",
              "      <td>145</td>\n",
              "      <td>152</td>\n",
              "      <td>156</td>\n",
              "      <td>138</td>\n",
              "      <td>135</td>\n",
              "      <td>137</td>\n",
              "      <td>125</td>\n",
              "      <td>136</td>\n",
              "      <td>151</td>\n",
              "      <td>141</td>\n",
              "      <td>134</td>\n",
              "      <td>133</td>\n",
              "      <td>132</td>\n",
              "      <td>131</td>\n",
              "      <td>133</td>\n",
              "      <td>138</td>\n",
              "      <td>146</td>\n",
              "      <td>149</td>\n",
              "      <td>153</td>\n",
              "      <td>156</td>\n",
              "      <td>...</td>\n",
              "      <td>125</td>\n",
              "      <td>128</td>\n",
              "      <td>134</td>\n",
              "      <td>140</td>\n",
              "      <td>142</td>\n",
              "      <td>138</td>\n",
              "      <td>134</td>\n",
              "      <td>121</td>\n",
              "      <td>130</td>\n",
              "      <td>142</td>\n",
              "      <td>125</td>\n",
              "      <td>136</td>\n",
              "      <td>138</td>\n",
              "      <td>151</td>\n",
              "      <td>142</td>\n",
              "      <td>145</td>\n",
              "      <td>147</td>\n",
              "      <td>144</td>\n",
              "      <td>137</td>\n",
              "      <td>129</td>\n",
              "      <td>127</td>\n",
              "      <td>128</td>\n",
              "      <td>136</td>\n",
              "      <td>145</td>\n",
              "      <td>148</td>\n",
              "      <td>141</td>\n",
              "      <td>134</td>\n",
              "      <td>115</td>\n",
              "      <td>125</td>\n",
              "      <td>138</td>\n",
              "      <td>125</td>\n",
              "      <td>142</td>\n",
              "      <td>146</td>\n",
              "      <td>153</td>\n",
              "      <td>137</td>\n",
              "      <td>131</td>\n",
              "      <td>138</td>\n",
              "      <td>146</td>\n",
              "      <td>153</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>125</td>\n",
              "      <td>132</td>\n",
              "      <td>135</td>\n",
              "      <td>137</td>\n",
              "      <td>147</td>\n",
              "      <td>151</td>\n",
              "      <td>136</td>\n",
              "      <td>113</td>\n",
              "      <td>116</td>\n",
              "      <td>142</td>\n",
              "      <td>157</td>\n",
              "      <td>142</td>\n",
              "      <td>122</td>\n",
              "      <td>115</td>\n",
              "      <td>120</td>\n",
              "      <td>123</td>\n",
              "      <td>131</td>\n",
              "      <td>132</td>\n",
              "      <td>131</td>\n",
              "      <td>127</td>\n",
              "      <td>157</td>\n",
              "      <td>150</td>\n",
              "      <td>135</td>\n",
              "      <td>125</td>\n",
              "      <td>135</td>\n",
              "      <td>149</td>\n",
              "      <td>150</td>\n",
              "      <td>139</td>\n",
              "      <td>126</td>\n",
              "      <td>144</td>\n",
              "      <td>153</td>\n",
              "      <td>146</td>\n",
              "      <td>138</td>\n",
              "      <td>137</td>\n",
              "      <td>134</td>\n",
              "      <td>127</td>\n",
              "      <td>124</td>\n",
              "      <td>131</td>\n",
              "      <td>135</td>\n",
              "      <td>131</td>\n",
              "      <td>...</td>\n",
              "      <td>148</td>\n",
              "      <td>144</td>\n",
              "      <td>139</td>\n",
              "      <td>135</td>\n",
              "      <td>135</td>\n",
              "      <td>141</td>\n",
              "      <td>148</td>\n",
              "      <td>162</td>\n",
              "      <td>162</td>\n",
              "      <td>154</td>\n",
              "      <td>139</td>\n",
              "      <td>129</td>\n",
              "      <td>127</td>\n",
              "      <td>129</td>\n",
              "      <td>129</td>\n",
              "      <td>129</td>\n",
              "      <td>132</td>\n",
              "      <td>135</td>\n",
              "      <td>137</td>\n",
              "      <td>139</td>\n",
              "      <td>137</td>\n",
              "      <td>144</td>\n",
              "      <td>156</td>\n",
              "      <td>157</td>\n",
              "      <td>151</td>\n",
              "      <td>155</td>\n",
              "      <td>167</td>\n",
              "      <td>173</td>\n",
              "      <td>171</td>\n",
              "      <td>161</td>\n",
              "      <td>146</td>\n",
              "      <td>139</td>\n",
              "      <td>142</td>\n",
              "      <td>141</td>\n",
              "      <td>136</td>\n",
              "      <td>127</td>\n",
              "      <td>129</td>\n",
              "      <td>132</td>\n",
              "      <td>133</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>126</td>\n",
              "      <td>132</td>\n",
              "      <td>134</td>\n",
              "      <td>128</td>\n",
              "      <td>123</td>\n",
              "      <td>121</td>\n",
              "      <td>119</td>\n",
              "      <td>117</td>\n",
              "      <td>110</td>\n",
              "      <td>102</td>\n",
              "      <td>104</td>\n",
              "      <td>113</td>\n",
              "      <td>113</td>\n",
              "      <td>94</td>\n",
              "      <td>76</td>\n",
              "      <td>69</td>\n",
              "      <td>65</td>\n",
              "      <td>57</td>\n",
              "      <td>45</td>\n",
              "      <td>47</td>\n",
              "      <td>140</td>\n",
              "      <td>122</td>\n",
              "      <td>111</td>\n",
              "      <td>118</td>\n",
              "      <td>126</td>\n",
              "      <td>121</td>\n",
              "      <td>110</td>\n",
              "      <td>104</td>\n",
              "      <td>110</td>\n",
              "      <td>102</td>\n",
              "      <td>99</td>\n",
              "      <td>104</td>\n",
              "      <td>99</td>\n",
              "      <td>82</td>\n",
              "      <td>69</td>\n",
              "      <td>66</td>\n",
              "      <td>71</td>\n",
              "      <td>65</td>\n",
              "      <td>52</td>\n",
              "      <td>52</td>\n",
              "      <td>...</td>\n",
              "      <td>76</td>\n",
              "      <td>78</td>\n",
              "      <td>80</td>\n",
              "      <td>71</td>\n",
              "      <td>44</td>\n",
              "      <td>38</td>\n",
              "      <td>56</td>\n",
              "      <td>36</td>\n",
              "      <td>45</td>\n",
              "      <td>58</td>\n",
              "      <td>52</td>\n",
              "      <td>41</td>\n",
              "      <td>60</td>\n",
              "      <td>89</td>\n",
              "      <td>99</td>\n",
              "      <td>117</td>\n",
              "      <td>134</td>\n",
              "      <td>150</td>\n",
              "      <td>156</td>\n",
              "      <td>66</td>\n",
              "      <td>71</td>\n",
              "      <td>82</td>\n",
              "      <td>86</td>\n",
              "      <td>70</td>\n",
              "      <td>43</td>\n",
              "      <td>44</td>\n",
              "      <td>63</td>\n",
              "      <td>36</td>\n",
              "      <td>50</td>\n",
              "      <td>66</td>\n",
              "      <td>60</td>\n",
              "      <td>58</td>\n",
              "      <td>91</td>\n",
              "      <td>123</td>\n",
              "      <td>126</td>\n",
              "      <td>126</td>\n",
              "      <td>143</td>\n",
              "      <td>161</td>\n",
              "      <td>167</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>5 rows Ã 401 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "   pixel1  pixel2  pixel3  pixel4  ...  pixel398  pixel399  pixel400  Label\n",
              "0      84      72      85     104  ...        99       114       130      1\n",
              "1     196     161     153     174  ...       113       129       159      0\n",
              "2     138     157     172     152  ...       138       146       153      0\n",
              "3     125     132     135     137  ...       129       132       133      0\n",
              "4     126     132     134     128  ...       143       161       167      1\n",
              "\n",
              "[5 rows x 401 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bYjrKxvKW6DI",
        "colab_type": "code",
        "outputId": "ac206d21-a8fd-4299-c998-0b6bfb8cc830",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "train_df.shape"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(1185, 401)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gfV0JR17tJhp",
        "colab_type": "text"
      },
      "source": [
        "What's inside in one of them..."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LjUIkIFuQ_X-",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 555
        },
        "outputId": "456f2feb-b998-4ffc-a6a6-bfeb640e24c9"
      },
      "source": [
        "train_df.iloc[0,0:].values"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([ 84,  72,  85, 104,  89,  59,  47,  48,  69,  88, 112, 137, 146,\n",
              "       124, 102, 102, 120, 120, 102,  86,  79,  70,  75,  89,  86,  69,\n",
              "        60,  64,  79,  84,  94, 115, 125, 106,  87,  92, 120, 121, 107,\n",
              "        92,  71,  66,  61,  69,  78,  69,  60,  66,  95,  92,  98, 118,\n",
              "       123,  90,  63,  63,  89,  97, 101,  98,  68,  69,  62,  64,  74,\n",
              "        65,  49,  49,  79,  84, 103, 129, 128,  80,  39,  35,  42,  58,\n",
              "        87, 104,  74,  75,  75,  79,  77,  61,  40,  32,  40,  53,  77,\n",
              "       103, 100,  57,  24,  27,  22,  34,  73, 104,  83,  75,  85,  91,\n",
              "        69,  46,  33,  22,  31,  40,  49,  58,  53,  27,  15,  27,  27,\n",
              "        25,  59,  92, 101,  76,  91,  96,  52,  26,  32,  24,  41,  45,\n",
              "        40,  33,  29,  18,  13,  23,  31,  18,  49,  87, 125,  86,  98,\n",
              "       101,  41,  19,  36,  33,  35,  43,  39,  31,  33,  27,  19,  21,\n",
              "        29,  13,  48,  94, 133, 107,  97,  96,  47,  18,  34,  32,  30,\n",
              "        32,  36,  31,  22,  29,  35,  25,  30,  32,  42, 103, 136, 114,\n",
              "       103,  97,  52,  26,  47,  44,  35,  32,  30,  22,  13,  22,  29,\n",
              "        20,  18,  25,  40, 104, 134, 122, 108,  92,  49,  29,  50,  46,\n",
              "        30,  30,  34,  30,  22,  26,  26,  12,  20,  27,  42, 106, 129,\n",
              "       131, 115,  89,  43,  23,  43,  41,  35,  35,  37,  32,  26,  31,\n",
              "        30,  15,  28,  29,  33,  84, 127, 136, 118,  88,  44,  23,  39,\n",
              "        40,  44,  40,  37,  27,  18,  27,  32,  22,  42,  38,  36,  77,\n",
              "       130, 133, 107,  79,  48,  26,  36,  37,  30,  38,  51,  49,  39,\n",
              "        37,  31,  18,  37,  40,  42,  88, 150, 137,  94,  70,  53,  34,\n",
              "        34,  33,  24,  44,  73,  82,  74,  67,  57,  37,  41,  48,  54,\n",
              "       100, 176, 148,  91,  70,  63,  44,  38,  34,  44,  60,  85,  92,\n",
              "        85,  91,  97,  88,  91,  93,  93, 133, 170, 135,  82,  73,  77,\n",
              "        57,  59,  65,  62,  78,  81,  74,  76,  83, 102, 127, 117, 118,\n",
              "       116, 112, 158, 135,  88,  77,  78,  61,  63,  65,  68,  80,  79,\n",
              "        69,  69,  71,  84, 104, 115, 115, 113, 107, 137, 128,  92,  80,\n",
              "        84,  74,  74,  67,  69,  75,  71,  62,  62,  60,  66,  81,  79,\n",
              "        83,  89,  92, 116, 118,  88,  80,  93,  94,  90,  70,  64,  67,\n",
              "        60,  56,  60,  58,  60,  72,  90,  99, 114, 130,   1])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b90Wwrgksgzp",
        "colab_type": "text"
      },
      "source": [
        "**Uhmm**...it doens't look like an useful data for keras. \n",
        "\n",
        "Let's now **reshape** for 20x20 as informed on kaggle description. Since we're going to put a single row as a vector of a picture, we're taking all the 400 columns and reshaping as an 20x20 with numpy's help. the last row are the labels.\n",
        "\n",
        "A wonderful explanation from the marvelous Stackoverflow:\n",
        "\n",
        "*Briefly, feature is input; label is output*.\n",
        "\n",
        "*A feature is one column of the data in your input set. For instance, if you're trying to predict the type of pet someone will choose, your input features might include age, home region, family income, etc. The label is the final choice, such as dog, fish, iguana, rock, etc.*\n",
        "\n",
        "*Once you've trained your model, you will give it sets of new input containing those features; it will return the predicted \"label\" (pet type) for that person* \n",
        "- by user: Prune - Systems Validation Engineer, Artificial Intelligence Products Group, Intel Corp."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d5AfRitWZw0x",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df_x = train_df.iloc[:,:-1].values.reshape(len(train_df),20,20,1)\n",
        "y = train_df.iloc[:,-1:].values"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qoq_okueurW6",
        "colab_type": "text"
      },
      "source": [
        "In our case, we just have two categories: Positive or Negative nucleus; so, num_classes is suposed to be only 2.\n",
        "Keras already has a function to do it, this is commonly known as 'one hot encoding' categories, since sometimes we have data that we can transform in number just to have an simple output, but it's not like we're going to make any calculation on it, just to make it simple, we're using even if we already have '1 or 0', educational purposes here."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6DkHbRO0bGfn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df_y = keras.utils.to_categorical(y, num_classes=2)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JCz13LtKbuM0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df_x = np.array(df_x)\n",
        "df_y = np.array(df_y)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1Mi7Z6PbenGM",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "1f7e0595-acb0-4665-e5d7-9b2ca3a8f52f"
      },
      "source": [
        "df_x.shape, df_y.shape"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((1185, 20, 20, 1), (1185, 2))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "57iTYFA2v8tM",
        "colab_type": "text"
      },
      "source": [
        "> ...The famous sklearn to help us split our dataset to train our model...\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oacqN4XJe7ni",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "x_train, x_test, y_train, y_test = train_test_split(df_x, df_y, test_size=0.25, random_state=10)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xTyf9GJQwS2n",
        "colab_type": "text"
      },
      "source": [
        "Now, our neural network: I've chosen the hard sigmoid function for activation on convolution because it was the best in outcome on this example, 20 nodes since each time I've set up higher numbers, the loss skyrocketed and compromised the accuracy evidently. Futhermore, sigmoid function for this input is good to sharpen up to the next layer, which I set up to softmax since we're dealing with two outputs: 1 or 0; and I want a better estimated probability.\n",
        "Also, the loss \"categorical_crossentropy\" is better for binary outputs. (I guess.)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WfhLF82NfcEC",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 141
        },
        "outputId": "95cc6b3e-bf21-46bf-f427-5ef0d77ebc6a"
      },
      "source": [
        "model = Sequential()\n",
        "model.add(Convolution2D(32, 3,data_format='channels_last', activation='hard_sigmoid', input_shape=(20,20,1)))\n",
        "model.add(MaxPooling2D(pool_size=(2,2)))\n",
        "model.add(Flatten())\n",
        "model.add(Dense(20))\n",
        "model.add(Dropout(.5))\n",
        "model.add(Dense(2))\n",
        "model.add(Activation('softmax'))\n",
        "model.compile(loss='categorical_crossentropy', optimizer='adadelta', metrics=['accuracy'])\n"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Colocations handled automatically by placer.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3445: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GWB4Lvbhjt2C",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 382
        },
        "outputId": "1e582735-b9d7-45d6-e8d7-cf1725bd13c9"
      },
      "source": [
        "model.summary()"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv2d_1 (Conv2D)            (None, 18, 18, 32)        320       \n",
            "_________________________________________________________________\n",
            "max_pooling2d_1 (MaxPooling2 (None, 9, 9, 32)          0         \n",
            "_________________________________________________________________\n",
            "flatten_1 (Flatten)          (None, 2592)              0         \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 20)                51860     \n",
            "_________________________________________________________________\n",
            "dropout_1 (Dropout)          (None, 20)                0         \n",
            "_________________________________________________________________\n",
            "dense_2 (Dense)              (None, 2)                 42        \n",
            "_________________________________________________________________\n",
            "activation_1 (Activation)    (None, 2)                 0         \n",
            "=================================================================\n",
            "Total params: 52,222\n",
            "Trainable params: 52,222\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nKsrbICW0UGC",
        "colab_type": "text"
      },
      "source": [
        "Let's run a single test with one epoch just for science purposes and not because I'm eager to test it..."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H2XUiTOiklZA",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 159
        },
        "outputId": "12f65806-7388-4c16-e117-b30a8a158294"
      },
      "source": [
        "model.fit(x_train[:100], y_train[:100], validation_data=(x_test[:20], y_test[:20]))"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.cast instead.\n",
            "Train on 100 samples, validate on 20 samples\n",
            "Epoch 1/1\n",
            "100/100 [==============================] - 0s 3ms/step - loss: 2.8495 - acc: 0.5200 - val_loss: 1.6363 - val_acc: 0.6000\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7fd7ba5ba1d0>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_tVXf4LFml5j",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 399
        },
        "outputId": "7b188308-2339-43b8-8b2c-573dd93a39c9"
      },
      "source": [
        "model.fit(x_train, y_train,epochs=10, validation_data=(x_test, y_test))"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 888 samples, validate on 297 samples\n",
            "Epoch 1/10\n",
            "888/888 [==============================] - 0s 412us/step - loss: 1.2265 - acc: 0.6971 - val_loss: 0.1145 - val_acc: 0.9663\n",
            "Epoch 2/10\n",
            "888/888 [==============================] - 0s 430us/step - loss: 0.2661 - acc: 0.9088 - val_loss: 0.0967 - val_acc: 0.9697\n",
            "Epoch 3/10\n",
            "888/888 [==============================] - 0s 417us/step - loss: 0.1147 - acc: 0.9673 - val_loss: 0.0435 - val_acc: 0.9899\n",
            "Epoch 4/10\n",
            "888/888 [==============================] - 0s 407us/step - loss: 0.1164 - acc: 0.9595 - val_loss: 0.0400 - val_acc: 0.9832\n",
            "Epoch 5/10\n",
            "888/888 [==============================] - 0s 433us/step - loss: 0.0993 - acc: 0.9628 - val_loss: 0.0333 - val_acc: 0.9933\n",
            "Epoch 6/10\n",
            "888/888 [==============================] - 0s 406us/step - loss: 0.0624 - acc: 0.9809 - val_loss: 0.0425 - val_acc: 0.9899\n",
            "Epoch 7/10\n",
            "888/888 [==============================] - 0s 409us/step - loss: 0.2064 - acc: 0.9414 - val_loss: 0.0242 - val_acc: 0.9933\n",
            "Epoch 8/10\n",
            "888/888 [==============================] - 0s 406us/step - loss: 0.0543 - acc: 0.9854 - val_loss: 0.0228 - val_acc: 0.9933\n",
            "Epoch 9/10\n",
            "888/888 [==============================] - 0s 412us/step - loss: 0.0594 - acc: 0.9809 - val_loss: 0.0570 - val_acc: 0.9865\n",
            "Epoch 10/10\n",
            "888/888 [==============================] - 0s 407us/step - loss: 0.0539 - acc: 0.9865 - val_loss: 0.0198 - val_acc: 0.9933\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7fd7ba5ba5c0>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-yUrWCgF073N",
        "colab_type": "text"
      },
      "source": [
        "Wow, 99,33% of accuracy, well, I think we've our winner."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KiT7rMmTl-C_",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "e83b9d73-5b63-401c-b882-376f469db454"
      },
      "source": [
        "print(\"print('Thank you for coming to my TED talk')\")"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "print('Thank you for coming to my TED talk')\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}